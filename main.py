import http.client
import json
import os
import time
import urllib.parse
from datetime import datetime, timedelta, timezone

# ================= é…ç½®è¯´æ˜ =================
# å»ºè®®ç”¨æˆ·åœ¨ä¸€ä¸ªåä¸º config.json çš„æ–‡ä»¶ä¸­å¡«å…¥ä»¥ä¸‹å†…å®¹ï¼š
# {
#   "apikey": "ä½ çš„_APIDANCE_KEY",
#   "authtoken": "ä½ çš„_TWITTER_AUTH_TOKEN"
# }
# æˆ–è€…ç›´æ¥ä¿®æ”¹ä¸‹æ–¹çš„ DEFAULT_CONFIG
# ===========================================

CONFIG_FILE = "config.json"
OUTPUT_FILE = f"crypto_daily_report_{datetime.now().strftime('%Y%m%d')}.txt"
MAX_PAGES = 30
TIME_LIMIT_HOURS = 24

def load_config():
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    
    # å¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·è¾“å…¥
    print("âš ï¸ æœªæ£€æµ‹åˆ° config.jsonï¼Œè¯·è¾“å…¥é…ç½®ä¿¡æ¯ï¼š")
    api_key = input("è¯·è¾“å…¥ APIDance API Key: ").strip()
    auth_token = input("è¯·è¾“å…¥ Twitter AuthToken: ").strip()
    
    # ä¿å­˜é…ç½®æ–¹ä¾¿ä¸‹æ¬¡ä½¿ç”¨
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump({"apikey": api_key, "authtoken": auth_token}, f, indent=4)
    
    return {"apikey": api_key, "authtoken": auth_token}

def parse_twitter_date(date_str):
    try:
        return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')
    except:
        return datetime.now(timezone.utc)

def fetch_page(api_key, auth_token, cursor=None):
    conn = http.client.HTTPSConnection("api.apidance.pro")
    headers = {
        'apikey': api_key,
        'AuthToken': auth_token,
        'Content-Type': 'application/json'
    }
    
    variables = {
        "count": 40,
        "includePromotedContent": False,
        "latestControlAvailable": True,
        "requestContext": "launch"
    }
    if cursor:
        variables["cursor"] = cursor

    encoded_vars = urllib.parse.quote(json.dumps(variables))
    url = f"/graphql/HomeLatestTimeline?variables={encoded_vars}"

    try:
        conn.request("GET", url, '', headers)
        res = conn.getresponse()
        return json.loads(res.read().decode("utf-8"))
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None

def run_task():
    config = load_config()
    api_key = config.get('apikey')
    auth_token = config.get('authtoken')

    if not api_key or not auth_token:
        print("âŒ é…ç½®ç¼ºå¤±ï¼Œæ— æ³•è¿è¡Œã€‚")
        return

    all_tweets = []
    next_cursor = None
    now = datetime.now(timezone.utc)
    time_limit = now - timedelta(hours=TIME_LIMIT_HOURS)

    print(f"\nğŸš€ å¼€å§‹æŠ“å–è¿‡å» {TIME_LIMIT_HOURS} å°æ—¶çš„æ¨æ–‡...")
    print(f"ğŸ“… æˆªæ­¢æ—¶é—´: {time_limit.strftime('%Y-%m-%d %H:%M:%S')}")

    for page in range(1, MAX_PAGES + 1):
        print(f"ğŸ“„ ç¬¬ {page} é¡µ...", end="", flush=True)
        data = fetch_page(api_key, auth_token, next_cursor)
        
        if not data:
            print(" (è¯·æ±‚å¤±è´¥æˆ–æ— æ•°æ®)")
            break

        instructions = data.get('data', {}).get('home', {}).get('home_timeline_urt', {}).get('instructions', [])
        page_tweets = []
        page_cursor = None
        reached_limit = False

        for instr in instructions:
            if instr.get('type') == 'TimelineAddEntries':
                for entry in instr.get('entries', []):
                    entry_id = entry.get('entryId', '')
                    content = entry.get('content', {})
                    
                    # è·å–æ¸¸æ ‡
                    if content.get('cursorType') == 'Bottom':
                        page_cursor = content.get('value')

                    # è§£ææ¨æ–‡
                    if entry_id.startswith('tweet-') or entry_id.startswith('home-conversation-'):
                        item_content = content.get('itemContent') or content.get('items', [{}])[0].get('item', {}).get('itemContent', {})
                        tweet_res = item_content.get('tweet_results', {}).get('result', {})
                        
                        if tweet_res and 'legacy' in tweet_res:
                            legacy = tweet_res['legacy']
                            created_at = parse_twitter_date(legacy.get('created_at'))
                            
                            if created_at < time_limit:
                                reached_limit = True
                            else:
                                author = tweet_res['core']['user_results']['result']['legacy']['name']
                                screen_name = tweet_res['core']['user_results']['result']['legacy']['screen_name']
                                text = legacy['full_text'].replace('\n', ' ')
                                tid = tweet_res['rest_id']
                                url = f"https://x.com/{screen_name}/status/{tid}"
                                
                                page_tweets.append(f"â° {created_at.strftime('%m-%d %H:%M')} | ğŸ‘¤ {author} (@{screen_name})\nğŸ“„ {text}\nğŸ”— {url}\n{'-'*50}")

        all_tweets.extend(page_tweets)
        print(f" âœ… è·å– {len(page_tweets)} æ¡")

        if reached_limit:
            print("ğŸ›‘ å·²è§¦è¾¾æ—¶é—´é™åˆ¶ï¼Œåœæ­¢æŠ“å–ã€‚")
            break
        
        if not page_cursor:
            print("âš ï¸ æ— æ›´å¤šé¡µé¢ã€‚")
            break
            
        next_cursor = page_cursor
        time.sleep(1.5)

    if all_tweets:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write(f"=== æ¯æ—¥ Crypto å…¨æ™¯æ¨æ–‡ (å…± {len(all_tweets)} æ¡) ===\n\n")
            f.write("\n".join(all_tweets))
        print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")
        print("ğŸ‘‰ ä¸‹ä¸€æ­¥ï¼šè¯·å°†è¯¥æ–‡ä»¶å†…å®¹å¤åˆ¶ç»™ ChatGPT/Claudeï¼Œå¹¶ä½¿ç”¨é…å¥—æç¤ºè¯ç”Ÿæˆç ”æŠ¥ã€‚")
    else:
        print("\nâŒ æœªè·å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ Token æ˜¯å¦è¿‡æœŸã€‚")

if __name__ == "__main__":
    run_task()
